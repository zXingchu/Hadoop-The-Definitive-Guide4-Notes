## 创建在线查询应用
* 气象数据集
    * 包含过去100多年上万个气象站的观测数据。数据集还在增长
* 构建一个简单的在线查询(不是批处理)界面，简单的命令行Java应用程序
    * 运行用户按时间顺序导航不同的观测站并浏览历史气象观测值
    * 假设数据集非常大，达到上亿条记录，且气象更新数据到达的速度很快——如从全球观测站收到超过每秒几百到几千次更新。必须能够及时显示观测数据，在收到数据1s后就能进行显示
* 第一个要求排除使用RDBMS
* 第二个要求排除HDFS。MapReduce作业可以用于建立索引以支持对观测数据进行随机访问，但HDFS和MapReduce并不擅长在有更新到达时维护索引
### 模式设计
* 两个表
    * stations 表
        * 该表包含观测站数据
        * 行的键是stationid
        * 有一个列族info
            * 能作为键/值字典来支持对观测站信息的查找
            * 字典的键就是列名info\:name，info\:location以及info\:description
    * observations 表
        * 表是静态的
        * 表存放气温观测数据
        * 行的键是stationid和逆序时间戳构成的组合键
            * 同一个观测站的数据被分组到一起，使用逆序时间戳(Long.MAX_VALUE - epoch)的二进制存储
        * 有一个列族data，包含一列airtemp，其值为观测到的气温值
* 模式选择取决于最高效的读取HBase的方式
    * 行和列以字典序升序保存
    * 二级索引和正则表达式匹配工具会损失其他性能
    * 查询数据最高效的方式对于选择最有效的存储和访问数据的设置非常关键
* 在shell环境中，可以用以下方式来定义表：
    ```
    hbase> create 'stations', {NAME => 'info'}
    hbase> create 'observations', {NAME => 'info'}
    ```
* 只对表单元格的最新版本感兴趣，所以把VERSION设为1(默认值为3)
* HBase中的宽表是指很多列较少行,即列多行少的表,一行中的数据量较大,行数少; 高表是指很多行较少列,即行多列少,一行中的数据量较少,行数大
* HBase 可以以极小的开销管理较宽的稀疏表
### 加载数据
* 观测站的数量相对较少，可以使用任何一种接口来插入观测站的静态数据
    * 示例代码中包含一个用于执行此操作的Java应用程序，运行方式如下：
        ```
        % hbase HBaseStationImporter input/ncdc/metadata/stations-fixed-idth.txt
        ```
* 观测数据
    * 要加载数十亿条。这种数据导入是一个极为复杂的过程，是一个需要长时间运行的数据库操作
    * MapReduce和HBase的分布式模型使可以充分利用集群
    * 通过把原始输入数据复制到HDFS，接着运行MapReduce作业，就能读到输入数据并将其写入HBase
* 一个MapReduce作业，将观测数据从前几章所用的输入文件导入HBase
    ```
		import java.io.IOException;
		import org.apache.hadoop.conf.Configured;
		import org.apache.hadoop.fs.Path;
		import org.apache.hadoop.hbase.HBaseConfiguration;
		import org.apache.hadoop.hbase.client.Put;
		import org.apache.hadoop.hbase.mapreduce.TableOutputFormat;
		import org.apache.hadoop.hbase.util.Bytes;
		import org.apache.hadoop.io.LongWritable;
		import org.apache.hadoop.io.Text;
		import org.apache.hadoop.mapreduce.Job;
		import org.apache.hadoop.mapreduce.Mapper;
		import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
		import org.apache.hadoop.util.Tool;
		import org.apache.hadoop.util.ToolRunner;

		/**
		 * Uses HBase's {@link TableOutputFormat} to load temperature data into a HBase table.
		 */
		public class HBaseTemperatureImporter extends Configured implements Tool {
		  
		  static class HBaseTemperatureMapper<K> extends Mapper<LongWritable, Text, K, Put> {
			private NcdcRecordParser parser = new NcdcRecordParser();

			@Override
			public void map(LongWritable key, Text value, Context context) throws
				IOException, InterruptedException {
			  parser.parse(value.toString());
			  if (parser.isValidTemperature()) {
				byte[] rowKey = RowKeyConverter.makeObservationRowKey(parser.getStationId(),
					parser.getObservationDate().getTime());
				Put p = new Put(rowKey);
				p.add(HBaseTemperatureQuery.DATA_COLUMNFAMILY,
					HBaseTemperatureQuery.AIRTEMP_QUALIFIER,
					Bytes.toBytes(parser.getAirTemperature()));
				context.write(null, p);
			  }
			}
		  }

		  @Override
		  public int run(String[] args) throws Exception {
			if (args.length != 1) {
			  System.err.println("Usage: HBaseTemperatureImporter <input>");
			  return -1;
			}
			Job job = new Job(getConf(), getClass().getSimpleName());
			job.setJarByClass(getClass());
			FileInputFormat.addInputPath(job, new Path(args[0]));
			job.getConfiguration().set(TableOutputFormat.OUTPUT_TABLE, "observations");
			job.setMapperClass(HBaseTemperatureMapper.class);
			job.setNumReduceTasks(0);
			job.setOutputFormatClass(TableOutputFormat.class);
			return job.waitForCompletion(true) ? 0 : 1;
		  }

		  public static void main(String[] args) throws Exception {
			int exitCode = ToolRunner.run(HBaseConfiguration.create(),
				new HBaseTemperatureImporter(), args);
			System.exit(exitCode);
		  }
		}
    ```
    * HBaseTemperatureImporter有一个名为HBaseTemperatureMapper的嵌套类。外部类实现了Tool，并对调用此map作业进行设置。HBaseTemperatureMapper解析时会检查输入是否有效，创建一个Put对象以便把有效的气温值添加到HBase的observations表的data\:airtemp列，使用了HBaseTemperatureQuery类中导出的data和airtemp静态常量。稍后介绍
    * 所用的行键在RowKeyConverter上的makeObservationRowKey()方法中，用观测站ID和观测时间创建
        * 使用HBase的Bytes类在字节数组和常用的Java类型之间进行转换
        * Bytes.SIZEOF_LONG常量用于计算行键字节数组的时间戳部分的大小
        * putBytes()和putLong()方法用于填充行键字节数组中的观测站ID和时间戳部分，使它们处于响应的偏移位置
        ```
					import org.apache.hadoop.hbase.util.Bytes;

					public class RowKeyConverter {

					  private static final int STATION_ID_LENGTH = 12;

					  /**
					   * @return A row key whose format is: <station_id> <reverse_order_timestamp>
					   */
					  public static byte[] makeObservationRowKey(String stationId,
						  long observationTime) {
						byte[] row = new byte[STATION_ID_LENGTH + Bytes.SIZEOF_LONG];
						Bytes.putBytes(row, 0, Bytes.toBytes(stationId), 0, STATION_ID_LENGTH);
						long reverseOrderTimestamp = Long.MAX_VALUE - observationTime;
						Bytes.putLong(row, STATION_ID_LENGTH, reverseOrderTimestamp);
						return row;
					  }
					}
        ```
     * 在run()方法中被配置为使用HBase的TableOutputFormat。将要写入的表必须在作业配置中通过设置TableOutputFormat.OUTPUTTABLE属性来指定
         * TableOutputFormat使用提供了方便，负责管理HTable实例的创建，否则需要在mapper的setup()方法中来做，还需要在cleanup()方法中调用close()
         * TableOutputFormat禁用了HTable的自动刷新功能，因此可以缓存对put()的调用以提高效率
#### 加载的分布
* 当心数据导入所引发的“步调一致”的情况
    * 所有的客户端都对同一个表的区域(在单个节点上）进行操作，然后再对下一个区域进行操作，依次进行
    * 加载操作没有均匀地分布在所有区域上
    * 通常是由“排序后输入”和切分的原理共同造成的
    * 在插入数据前，针对行键按数据排列的次序进行随机处理，可能有助于减少这种情况
* 新表只有一个区域，所有更新都会加载到这个区域，知道分裂为止
    * 意味着上传数据在开始时比较慢，直到有足够多的区域被分布到各个节点
* 可以通过批量加载避免上述问题
#### 批量加载
* HBase有高效的“批量加载”(bulk loading)工具
    * 从MapReduce把以内部格式表示的数据直接写入文件系统，从而实现批量加载
    * 顺着这条路，加载HBase实例的速度比用HBase客户端API写入数据的方式至少快一个数量级
* 批量加载的过程分为两步
    * 第一步，使用HFileOutputFormat2通过一个MapReduce作业将HFiles写入HDFS目录
        * 由于数据行必须按顺序写入，该作业需要执行行键的完全排序
        * HFileOutputFormat2的configureIncrementalLoad方法可以完成所有必要的配置
    * 第二步，涉及将HFiles从HDFS移入现有的HBase表中
        * 这张表在此过程中可以是活跃的
### 在线查询
* 为了实现在线查询应用，直接使用HBase的Java API
#### 观测站信息查询
* 最简单的查询就是获取静态的观测站信息
    * 单行查找，get()操作
    * HBase提供了额外的控制功能和灵活性，这里讲info列族作为键/值字典(列名作为键，列值作为值)

```
import java.io.IOException;
import java.util.LinkedHashMap;
import java.util.Map;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.client.Get;
import org.apache.hadoop.hbase.client.HTable;
import org.apache.hadoop.hbase.client.Result;
import org.apache.hadoop.hbase.util.Bytes;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

public class HBaseStationQuery extends Configured implements Tool {
  static final byte[] INFO_COLUMNFAMILY = Bytes.toBytes("info");
  static final byte[] NAME_QUALIFIER = Bytes.toBytes("name");
  static final byte[] LOCATION_QUALIFIER = Bytes.toBytes("location");
  static final byte[] DESCRIPTION_QUALIFIER = Bytes.toBytes("description");

  public Map<String, String> getStationInfo(HTable table, String stationId)
      throws IOException {
    Get get = new Get(Bytes.toBytes(stationId));
    get.addFamily(INFO_COLUMNFAMILY);
    Result res = table.get(get);
    if (res == null) {
      return null;
    }
    Map<String, String> resultMap = new LinkedHashMap<String, String>();
    resultMap.put("name", getValue(res, INFO_COLUMNFAMILY, NAME_QUALIFIER));
    resultMap.put("location", getValue(res, INFO_COLUMNFAMILY,
        LOCATION_QUALIFIER));
    resultMap.put("description", getValue(res, INFO_COLUMNFAMILY,
        DESCRIPTION_QUALIFIER));
    return resultMap;
  }

  private static String getValue(Result res, byte[] cf, byte[] qualifier) {
    byte[] value = res.getValue(cf, qualifier);
    return value == null? "": Bytes.toString(value);
  }

  public int run(String[] args) throws IOException {
    if (args.length != 1) {
      System.err.println("Usage: HBaseStationQuery <station_id>");
      return -1;
    }

    HTable table = new HTable(HBaseConfiguration.create(getConf()), "stations");
    try {
      Map<String, String> stationInfo = getStationInfo(table, args[0]);
      if (stationInfo == null) {
        System.err.printf("Station ID %s not found.\n", args[0]);
        return -1;
      }
      for (Map.Entry<String, String> station : stationInfo.entrySet()) {
        System.out.printf("%s\t%s\n", station.getKey(), station.getValue());
      }
      return 0;
    } finally {
      table.close();
    }
  }

  public static void main(String[] args) throws Exception {
    int exitCode = ToolRunner.run(HBaseConfiguration.create(),
        new HBaseStationQuery(), args);
    System.exit(exitCode);
  }
}
```
* getStationObservations()接受一个HTable实例和一个观测站ID
* 为了获取观测站的信息，使用HTable.get()来传递一个Get实例
    * 此Get实例被设置为用于获取已定义INFO_COLUMNFAMILY中由观测站ID所指明的列的值
* get()的结果在Result中返回

```
import org.apache.hadoop.hbase.client.Connection;
import org.apache.hadoop.hbase.client.ConnectionFactory;

/**
 * HBase 1.0 version of HBaseStationQuery that uses {@code Connection},
 * and {@code Table}.
 */
  public int run(String[] args) throws IOException {
    if (args.length != 1) {
      System.err.println("Usage: HBaseStationQuery <station_id>");
      return -1;
    }

    Configuration config = HBaseConfiguration.create();
    Connection connection = ConnectionFactory.createConnection(config);
    try {
      TableName tableName = TableName.valueOf("stations");
      Table table = connection.getTable(tableName);
      try {
        Map<String, String> stationInfo = getStationInfo(table, args[0]);
        if (stationInfo == null) {
          System.err.printf("Station ID %s not found.\n", args[0]);
          return -1;
        }
        for (Map.Entry<String, String> station : stationInfo.entrySet()) {
          System.out.printf("%s\t%s\n", station.getKey(), station.getValue());
        }
        return 0;
      } finally {
        table.close();
      }
    } finally {
      connection.close();
    }
  }
```
#### 观测数据查询
* 对observations表的查询需要输入的参数包括站点ID，开始时间以及要返回的最大行数
* 由于数据行是按照观测站以观测时间逆序存储的，因此查询将返回发生在开始时间之后的观察值

```
import java.io.IOException;
import java.util.Map;
import java.util.NavigableMap;
import java.util.TreeMap;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.client.HTable;
import org.apache.hadoop.hbase.client.Result;
import org.apache.hadoop.hbase.client.ResultScanner;
import org.apache.hadoop.hbase.client.Scan;
import org.apache.hadoop.hbase.util.Bytes;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

public class HBaseTemperatureQuery extends Configured implements Tool {
  static final byte[] DATA_COLUMNFAMILY = Bytes.toBytes("data");
  static final byte[] AIRTEMP_QUALIFIER = Bytes.toBytes("airtemp");
  
  public NavigableMap<Long, Integer> getStationObservations(HTable table,
      String stationId, long maxStamp, int maxCount) throws IOException {
    byte[] startRow = RowKeyConverter.makeObservationRowKey(stationId, maxStamp);
    NavigableMap<Long, Integer> resultMap = new TreeMap<Long, Integer>();
    Scan scan = new Scan(startRow);
    scan.addColumn(DATA_COLUMNFAMILY, AIRTEMP_QUALIFIER);
    ResultScanner scanner = table.getScanner(scan);
    try {
      Result res;
      int count = 0;
      while ((res = scanner.next()) != null && count++ < maxCount) {
        byte[] row = res.getRow();
        byte[] value = res.getValue(DATA_COLUMNFAMILY, AIRTEMP_QUALIFIER);
        Long stamp = Long.MAX_VALUE -
          Bytes.toLong(row, row.length - Bytes.SIZEOF_LONG, Bytes.SIZEOF_LONG);
        Integer temp = Bytes.toInt(value);
        resultMap.put(stamp, temp);
      }
    } finally {
      scanner.close();
    }
    return resultMap;
  }

  public int run(String[] args) throws IOException {
    if (args.length != 1) {
      System.err.println("Usage: HBaseTemperatureQuery <station_id>");
      return -1;
    }
    
    HTable table = new HTable(HBaseConfiguration.create(getConf()), "observations");
    try {
      NavigableMap<Long, Integer> observations =
          getStationObservations(table, args[0], Long.MAX_VALUE, 10).descendingMap();
      for (Map.Entry<Long, Integer> observation : observations.entrySet()) {
        // Print the date, time, and temperature
        System.out.printf("%1$tF %1$tR\t%2$s\n", observation.getKey(),
            observation.getValue());
      }
      return 0;
    } finally {
      table.close();
    }
  }

  public static void main(String[] args) throws Exception {
    int exitCode = ToolRunner.run(HBaseConfiguration.create(),
        new HBaseTemperatureQuery(), args);
    System.exit(exitCode);
  }
}

```
* getStationObservations()方法使用HBase扫描器对表行进行遍历
    * 返回一个NavigableMap\<Long, Integer\>，键为时间戳，值是温度。此map按键的升序来排序，数据按时间顺序排列
* run()方法调用getStationObservations()以请求最近的10个观察值，并通过调用descendingMap()使返回值仍然回归到降序
* HBase 0.98新增反向扫描的能力，可按时间顺序存储观察值，并从给定的起始行开始反向扫描。反向扫描比正向扫描要慢几个百分点
* 使用反向扫描之前调用Scan对象的setReversed(true)方法

```
import org.apache.hadoop.hbase.client.Connection;
import org.apache.hadoop.hbase.client.ConnectionFactory;
  public int run(String[] args) throws IOException {
    if (args.length != 1) {
      System.err.println("Usage: HBaseTemperatureQuery <station_id>");
      return -1;
    }

    Configuration config = HBaseConfiguration.create();
    Connection connection = ConnectionFactory.createConnection(config);
    try {
      TableName tableName = TableName.valueOf("observations");
      Table table = connection.getTable(tableName);
      try {
        NavigableMap<Long, Integer> observations =
            getStationObservations(table, args[0], Long.MAX_VALUE, 10).descendingMap();
        for (Map.Entry<Long, Integer> observation : observations.entrySet()) {
          // Print the date, time, and temperature
          System.out.printf("%1$tF %1$tR\t%2$s\n", observation.getKey(),
              observation.getValue());
        }
        return 0;
      } finally {
        table.close();
      }
    } finally {
      connection.close();
    }
  }
```